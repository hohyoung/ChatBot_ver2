from __future__ import annotations

import re
from typing import List, Tuple, Any, Dict, Union, AsyncIterator, Optional

from app.services.openai_client import (
    get_client,
    get_async_client,
    call_chat_completion_async,
    call_chat_completion_stream_async,
    _get_semaphore,
)
from app.config import settings
from app.services.logging import get_logger
from app.models.schemas import Chunk, ScoredChunk  # â† ê²½ë¡œ ì£¼ì˜!

log = get_logger("app.rag.generator")


def _filter_actually_used_chunks(
    answer: str,
    chunks: List[Chunk],
    min_overlap_ratio: float = 0.15,
) -> List[Chunk]:
    """
    LLM ë‹µë³€ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ëœ ì²­í¬ë§Œ í•„í„°ë§.

    ì „ëµ:
    1. ì²­í¬ì˜ í•µì‹¬ í‚¤ì›Œë“œ/ë¬¸êµ¬ê°€ ë‹µë³€ì— ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸
    2. ì²­í¬ ë‚´ìš©ê³¼ ë‹µë³€ì˜ n-gram ì˜¤ë²„ë© ë¹„ìœ¨ ê³„ì‚°
    3. íŠ¹ì • ì„ê³„ê°’ ì´ìƒì¸ ì²­í¬ë§Œ ë°˜í™˜

    Args:
        answer: LLMì´ ìƒì„±í•œ ë‹µë³€
        chunks: LLMì— ì „ë‹¬ëœ ì²­í¬ ëª©ë¡
        min_overlap_ratio: ìµœì†Œ ì˜¤ë²„ë© ë¹„ìœ¨ (ê¸°ë³¸ 0.15 = 15%)

    Returns:
        ì‹¤ì œ ë‹µë³€ì— ì‚¬ìš©ëœ ê²ƒìœ¼ë¡œ íŒë‹¨ë˜ëŠ” ì²­í¬ ëª©ë¡
    """
    if not answer or not chunks:
        return chunks

    # ë‹µë³€ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)
    answer_normalized = re.sub(r'\s+', ' ', answer.lower().strip())

    used_chunks = []

    for chunk in chunks:
        content = chunk.content or ""
        if not content.strip():
            continue

        # ì²­í¬ ë‚´ìš© ì •ê·œí™”
        content_normalized = re.sub(r'\s+', ' ', content.lower().strip())

        # 1) í•µì‹¬ ë¬¸êµ¬ ë§¤ì¹­ (3ë‹¨ì–´ ì´ìƒ ì—°ì† ì¼ì¹˜)
        # ì²­í¬ì—ì„œ ì˜ë¯¸ìˆëŠ” ë¬¸êµ¬ ì¶”ì¶œ (ìˆ«ì+ë‹¨ìœ„, ì¡°í•­ëª… ë“±)
        phrases = _extract_key_phrases(content)
        phrase_match = any(
            phrase.lower() in answer_normalized
            for phrase in phrases
            if len(phrase) >= 4  # ìµœì†Œ 4ì ì´ìƒ
        )

        # 2) ë‹¨ì–´ ì˜¤ë²„ë© ê³„ì‚°
        chunk_words = set(re.findall(r'[ê°€-í£a-zA-Z0-9]+', content_normalized))
        answer_words = set(re.findall(r'[ê°€-í£a-zA-Z0-9]+', answer_normalized))

        if chunk_words:
            overlap = chunk_words & answer_words
            # ë¶ˆìš©ì–´ ì œì™¸ (ì¡°ì‚¬, ì¼ë°˜ ìš©ì–´)
            stopwords = {'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ', 'ì™€', 'ê³¼',
                         'ë°', 'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê²½ìš°', 'ëŒ€í•´', 'ê´€ë ¨', 'í•´ë‹¹',
                         'the', 'a', 'an', 'is', 'are', 'of', 'to', 'in', 'for'}
            meaningful_overlap = overlap - stopwords
            meaningful_chunk_words = chunk_words - stopwords

            if meaningful_chunk_words:
                overlap_ratio = len(meaningful_overlap) / len(meaningful_chunk_words)
            else:
                overlap_ratio = 0.0
        else:
            overlap_ratio = 0.0

        # 3) ì¡°í•­/ê·œì • ë²ˆí˜¸ ë§¤ì¹­ (ì˜ˆ: "ì œ10ì¡°", "ì œ3í•­")
        regulation_pattern = r'ì œ\s*\d+\s*[ì¡°í•­í˜¸]'
        chunk_regulations = set(re.findall(regulation_pattern, content))
        answer_regulations = set(re.findall(regulation_pattern, answer))
        regulation_match = bool(chunk_regulations & answer_regulations)

        # 4) ìˆ«ì+ë‹¨ìœ„ ë§¤ì¹­ (ì˜ˆ: "15ì¼", "80%", "1ë…„")
        number_pattern = r'\d+(?:\.\d+)?(?:ì¼|ê°œì›”|ë…„|%|ì›|ì‹œê°„|ë¶„)'
        chunk_numbers = set(re.findall(number_pattern, content))
        answer_numbers = set(re.findall(number_pattern, answer))
        number_match = bool(chunk_numbers & answer_numbers)

        # ì¢…í•© íŒë‹¨
        is_used = (
            phrase_match or
            regulation_match or
            number_match or
            overlap_ratio >= min_overlap_ratio
        )

        if is_used:
            used_chunks.append(chunk)
            log.debug(
                f"[FILTER] ì‚¬ìš©ë¨: {chunk.chunk_id} "
                f"(phrase={phrase_match}, reg={regulation_match}, "
                f"num={number_match}, overlap={overlap_ratio:.2f})"
            )
        else:
            log.debug(
                f"[FILTER] ë¯¸ì‚¬ìš©: {chunk.chunk_id} "
                f"(phrase={phrase_match}, reg={regulation_match}, "
                f"num={number_match}, overlap={overlap_ratio:.2f})"
            )

    # ìµœì†Œ 1ê°œëŠ” ë°˜í™˜ (fallback)
    if not used_chunks and chunks:
        used_chunks = [chunks[0]]
        log.warning("[FILTER] ì‚¬ìš©ëœ ì²­í¬ ì—†ìŒ, ì²« ë²ˆì§¸ ì²­í¬ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©")

    log.info(f"[FILTER] {len(chunks)}ê°œ ì²­í¬ â†’ {len(used_chunks)}ê°œ ì‹¤ì œ ì‚¬ìš©")

    return used_chunks


def _extract_key_phrases(text: str, min_len: int = 4, max_phrases: int = 20) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸êµ¬ ì¶”ì¶œ.
    - ì¡°í•­ëª… (ì œNì¡°, ì œNí•­)
    - ìˆ«ì+ë‹¨ìœ„ (15ì¼, 80%)
    - ê³ ìœ ëª…ì‚¬/ì „ë¬¸ìš©ì–´ (ì—°ì°¨íœ´ê°€, ì¶œì¥ë¹„ ë“±)
    """
    phrases = []

    # 1) ì¡°í•­ëª…
    regulations = re.findall(r'ì œ\s*\d+\s*[ì¡°í•­í˜¸][ì˜\s]*\d*', text)
    phrases.extend(regulations)

    # 2) ìˆ«ì+ë‹¨ìœ„ í‘œí˜„
    numbers = re.findall(r'\d+(?:\.\d+)?(?:ì¼|ê°œì›”|ë…„|%|ì›|ì‹œê°„|ë¶„|ëª…|ê°œ|íšŒ)', text)
    phrases.extend(numbers)

    # 3) í•œê¸€ ë³µí•©ì–´ (2ì–´ì ˆ ì´ìƒ)
    # ì˜ˆ: "ì—°ì°¨íœ´ê°€", "ì¶œì¥ë¹„", "ì¸ì‚¬ìœ„ì›íšŒ"
    compound_words = re.findall(r'[ê°€-í£]{2,}(?:íœ´ê°€|ê·œì •|ìœ„ì›íšŒ|ìˆ˜ë‹¹|ë¹„ìš©|ì§€ì›|ì‹ ì²­|ìŠ¹ì¸|ê¸°ì¤€)', text)
    phrases.extend(compound_words)

    # 4) ê´„í˜¸ ì•ˆ ìš©ì–´
    parenthetical = re.findall(r'\(([^)]{2,20})\)', text)
    phrases.extend(parenthetical)

    # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ í•„í„°
    unique_phrases = []
    seen = set()
    for p in phrases:
        p_clean = p.strip()
        if p_clean and len(p_clean) >= min_len and p_clean not in seen:
            seen.add(p_clean)
            unique_phrases.append(p_clean)
            if len(unique_phrases) >= max_phrases:
                break

    return unique_phrases


def _score_of(sc: Union[ScoredChunk, Chunk]) -> float:
    """ì •ë ¬ ì ìˆ˜ í†µì¼: ScoredChunkë©´ final_score/score/similarity/(1-distance) ìš°ì„ ìˆœìœ„ ì‚¬ìš©, Chunkë©´ 0."""
    if isinstance(sc, ScoredChunk):
        # pydantic v2: hasattrë¡œ ì ‘ê·¼
        # 1. final_score (rerankerì—ì„œ ì„¤ì •í•˜ëŠ” ìµœì¢… ì ìˆ˜)
        if hasattr(sc, "final_score") and isinstance(sc.final_score, (int, float)):
            return float(sc.final_score)
        # 2. score (deprecated, í•˜ìœ„ í˜¸í™˜ìš©)
        if hasattr(sc, "score") and isinstance(sc.score, (int, float)):
            return float(sc.score)
        # 3. similarity
        if hasattr(sc, "similarity") and isinstance(sc.similarity, (int, float)):
            return float(sc.similarity)
        # 4. distance â†’ similarityë¡œ ë³€í™˜
        if hasattr(sc, "distance") and isinstance(sc.distance, (int, float)):
            d = max(0.0, min(1.0, float(sc.distance)))
            return 1.0 - d
    return 0.0


def _as_chunk(x: Union[ScoredChunk, Chunk]) -> Chunk:
    """ScoredChunk â†’ Chunk, ì´ë¯¸ Chunkë©´ ê·¸ëŒ€ë¡œ."""
    if isinstance(x, ScoredChunk):
        return x.chunk
    return x


def _select_chunks(
    candidates: List[Union[ScoredChunk, Chunk]],
    max_chars: int = 6000,
    min_score: float = 0.05,  # ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜
    max_docs: int = 3,  # ìµœëŒ€ ë¬¸ì„œ ìˆ˜
    max_chunks_per_doc: int = 2,  # ë¬¸ì„œë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜
) -> List[Chunk]:
    """
    ì»¨í…ìŠ¤íŠ¸ì— ë„£ì„ ì²­í¬ ì„ ë³„.
    - ì ìˆ˜ê°€ min_score ë¯¸ë§Œì¸ ì²­í¬ëŠ” ì œì™¸
    - ë¬¸ì„œ ë‹¨ìœ„ë¡œ ìµœëŒ€ max_docsê°œë§Œ ì„ íƒ
    - ê° ë¬¸ì„œì—ì„œ ìµœëŒ€ max_chunks_per_docê°œ ì²­í¬ ì„ íƒ (í‘œ/ë³¸ë¬¸ ëª¨ë‘ í¬í•¨)
    - ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ max_chars ì œí•œ
    ë°˜í™˜ì€ í•­ìƒ List[Chunk]
    """
    # [DEBUG] ì…ë ¥ëœ ëª¨ë“  í›„ë³´ ì²­í¬ ë¡œê¹…
    log.info(f"[_select_chunks] ===== í›„ë³´ ì²­í¬ ëª©ë¡ ({len(candidates)}ê°œ) =====")
    for i, c in enumerate(candidates):
        ch = _as_chunk(c)
        score = _score_of(c)
        content_preview = (ch.content or "")[:100].replace("\n", " ")
        log.info(f"[_select_chunks] í›„ë³´ {i+1}: chunk_id={ch.chunk_id}, doc_title={ch.doc_title}, score={score:.4f}")
        log.info(f"[_select_chunks]   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}...")

    # 1ë‹¨ê³„: ìµœì†Œ ì ìˆ˜ í•„í„°ë§
    filtered = []
    for c in candidates:
        score = _score_of(c)
        if score >= min_score:
            filtered.append((c, score))
        else:
            ch = _as_chunk(c)
            log.info(f"[_select_chunks] ì ìˆ˜ ë¯¸ë‹¬ë¡œ ì œì™¸: chunk_id={ch.chunk_id}, score={score:.4f} < {min_score}")

    if not filtered:
        # í•„í„°ë§ í›„ ì²­í¬ê°€ ì—†ìœ¼ë©´, ê°€ì¥ ì ìˆ˜ ë†’ì€ ê²ƒ í•˜ë‚˜ë¼ë„ í¬í•¨
        if candidates:
            best = max(candidates, key=_score_of)
            filtered = [(best, _score_of(best))]
            log.warning(f"[_select_chunks] ëª¨ë“  ì²­í¬ê°€ ì ìˆ˜ ë¯¸ë‹¬, ìµœê³  ì ìˆ˜ ì²­í¬ 1ê°œ ì‚¬ìš©")

    # 2ë‹¨ê³„: ì ìˆ˜ ìˆœ ì •ë ¬
    filtered.sort(key=lambda x: x[1], reverse=True)

    # 3ë‹¨ê³„: ë¬¸ì„œ ë‹¨ìœ„ ì²­í¬ ì œí•œ (ê° ë¬¸ì„œì—ì„œ max_chunks_per_docê°œê¹Œì§€ í—ˆìš©)
    log.info(f"[_select_chunks] ===== ë¬¸ì„œ ë‹¨ìœ„ ì²­í¬ ì„ íƒ ì‹œì‘ (max_chunks_per_doc={max_chunks_per_doc}) =====")
    doc_chunk_count: Dict[str, int] = {}  # doc_id â†’ ì„ íƒëœ ì²­í¬ ìˆ˜
    deduplicated = []
    for c, score in filtered:
        ch = _as_chunk(c)
        doc_id = ch.doc_id
        current_count = doc_chunk_count.get(doc_id, 0)

        if current_count < max_chunks_per_doc:
            # ì´ ë¬¸ì„œì—ì„œ ì•„ì§ max_chunks_per_docê°œ ë¯¸ë§Œ ì„ íƒë¨
            doc_chunk_count[doc_id] = current_count + 1
            deduplicated.append((ch, score))
            content_preview = (ch.content or "")[:80].replace("\n", " ")
            log.info(f"[_select_chunks] ì„ íƒë¨: chunk_id={ch.chunk_id}, score={score:.4f} (ë¬¸ì„œ ë‚´ {current_count + 1}ë²ˆì§¸)")
            log.info(f"[_select_chunks]   ë‚´ìš©: {content_preview}...")

            # max_docs ë¬¸ì„œ ìˆ˜ ì œí•œ í™•ì¸
            if len(doc_chunk_count) >= max_docs and all(
                cnt >= max_chunks_per_doc for cnt in doc_chunk_count.values()
            ):
                log.info(f"[_select_chunks] max_docs={max_docs} ë¬¸ì„œì—ì„œ ê°ê° ìµœëŒ€ ì²­í¬ ë„ë‹¬, ì¤‘ë‹¨")
                break
        else:
            content_preview = (ch.content or "")[:80].replace("\n", " ")
            log.info(f"[_select_chunks] ì œì™¸ë¨ (ë¬¸ì„œë‹¹ {max_chunks_per_doc}ê°œ ì´ˆê³¼): chunk_id={ch.chunk_id}, doc_id={doc_id}, score={score:.4f}")
            log.info(f"[_select_chunks]   ë‚´ìš©: {content_preview}...")

    # 4ë‹¨ê³„: max_chars ì œí•œ ì ìš©
    picked: List[Chunk] = []
    total = 0
    for ch, score in deduplicated:
        text = ch.content or ""
        l = len(text)
        if l == 0:
            continue
        if total + l > max_chars and picked:
            break
        picked.append(ch)
        total += l
        log.debug(f"[_select_chunks] ì„ íƒ: {ch.doc_title} (score={score:.3f}, chars={l})")

    log.info(f"[_select_chunks] {len(candidates)}ê°œ í›„ë³´ â†’ {len(picked)}ê°œ ì„ íƒ (min_score={min_score}, max_docs={max_docs})")

    return picked


def _build_context(chunks: List[Chunk]) -> Tuple[str, List[Dict[str, str]]]:
    """
    í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ êµ¬ì„±.

    Returns:
        (context_string, image_refs)
        - context_string: ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        - image_refs: [{"ref": "[ì´ë¯¸ì§€1]", "url": "/static/images/...", "type": "table/figure"}]
    """
    lines: List[str] = []
    image_refs: List[Dict[str, str]] = []
    img_counter = 1

    for i, ch in enumerate(chunks, start=1):
        title = ch.doc_title or ch.doc_id or "Untitled"
        header = f"[{i}] {title} (doc_type={ch.doc_type}, visibility={ch.visibility}, tags={','.join(ch.tags or [])})"

        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ì²­í¬ì¸ ê²½ìš° ì´ë¯¸ì§€ ì°¸ì¡° ì¶”ê°€
        if getattr(ch, 'has_image', False) and getattr(ch, 'image_url', None):
            img_type = getattr(ch, 'image_type', 'image')
            img_ref = f"[ì´ë¯¸ì§€{img_counter}]"
            image_refs.append({
                "ref": img_ref,
                "url": ch.image_url,
                "type": img_type,
                "doc_title": title,
                "page": getattr(ch, 'page_start', None),
            })
            header += f" {img_ref} ğŸ“Šì›ë³¸ì´ë¯¸ì§€ìˆìŒ"
            img_counter += 1

        lines.append(header)
        lines.append(ch.content.strip())
        lines.append("")  # ë¹ˆ ì¤„

    return "\n".join(lines).strip(), image_refs


def _get_system_prompt(image_refs: List[Dict[str, str]] = None) -> str:
    """
    ì±—ë´‡ í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸.
    í˜ë¥´ì†Œë‚˜: ì¹œì ˆí•œ ì‚¬ë‚´ ê·œì • ìƒë‹´ì›

    Args:
        image_refs: ì´ë¯¸ì§€ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸ [{"ref": "[ì´ë¯¸ì§€1]", "url": "...", "type": "table"}]
    """
    base_prompt = (
        "ë‹¹ì‹ ì€ íšŒì‚¬ì˜ **ì¹œì ˆí•œ ê·œì • ìƒë‹´ì›**ì…ë‹ˆë‹¤.\n"
        "ì§ì›ë¶„ë“¤ì´ í¸í•˜ê²Œ ì§ˆë¬¸í•  ìˆ˜ ìˆë„ë¡ ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¡œ ì•ˆë‚´í•´ ë“œë¦¬ëŠ” ê²ƒì´ ëª©í‘œì˜ˆìš”.\n\n"

        "## ë§íˆ¬ ê°€ì´ë“œ\n"
        "- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë˜, ë”±ë”±í•˜ì§€ ì•Šê³  ë¶€ë“œëŸ½ê²Œ ë§í•´ì£¼ì„¸ìš”.\n"
        "- '~ì…ë‹ˆë‹¤', '~ë©ë‹ˆë‹¤' ëŒ€ì‹  '~ì˜ˆìš”', '~ì´ì—ìš”', '~ë“œë ¤ìš”' ê°™ì€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "- ê³µê°ê³¼ ë°°ë ¤ì˜ í‘œí˜„ì„ ì ì ˆíˆ ë„£ì–´ì£¼ì„¸ìš”. (ì˜ˆ: 'ê¶ê¸ˆí•˜ì…¨ì£ ?', 'ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”')\n"
        "- ë„ˆë¬´ ê³¼í•˜ê²Œ ì¹œê·¼í•˜ê±°ë‚˜ ê°€ë³ì§€ ì•Šê²Œ, ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ë©´ì„œ ë”°ëœ»í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"

        "## âš ï¸ ê°€ì¥ ì¤‘ìš”í•œ ì›ì¹™: í™˜ê° ê¸ˆì§€\n"
        "**ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.**\n"
        "- ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ìˆëŠ” ì •ë³´ë§Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "- 'ì¼ë°˜ì ìœ¼ë¡œ', 'ë³´í†µ', 'ëŒ€ê°œ' ê°™ì€ ì¶”ì¸¡ì„± í‘œí˜„ìœ¼ë¡œ ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.\n"
        "- ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”'ë¼ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.\n"
        "- ë‹´ë‹¹ ë¶€ì„œë‚˜ ë‹´ë‹¹ì ì—°ë½ì²˜ê°€ ë¬¸ì„œì— ìˆë‹¤ë©´ í•´ë‹¹ ë¶€ì„œì— ë¬¸ì˜í•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”.\n\n"

        "## ì‘ë‹µ ì›ì¹™\n"
        "1. **ê·¼ê±° í•„ìˆ˜**: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì§ì ‘ ì¸ìš© ê°€ëŠ¥í•´ì•¼ í•´ìš”.\n"
        "2. **ì¡°í•­ ì•ˆë‚´**: ê´€ë ¨ ê·œì •ì´ë‚˜ ì¡°í•­ì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´í•´ ë“œë ¤ìš”. (ì˜ˆ: 'ì œ10ì¡°ì— ë”°ë¥´ë©´~')\n"
        "3. **ì‰¬ìš´ ì„¤ëª…**: ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ ë“œë ¤ìš”.\n"
        "4. **êµ¬ì¡°í™”**: ë‚´ìš©ì´ ë§ì„ ë•ŒëŠ” ì½ê¸° ì‰½ê²Œ ì •ë¦¬í•´ì„œ ì•Œë ¤ë“œë ¤ìš”.\n"
        "5. **ë¶ˆí™•ì‹¤ì„±**: ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ 'ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•Šì•„ìš”'ë¼ê³  ì†”ì§íˆ ë§ì”€ë“œë¦¬ê³ , ë‹´ë‹¹ ë¶€ì„œ ë¬¸ì˜ë¥¼ ì•ˆë‚´í•´ìš”.\n\n"

        "## ğŸ§  ë‹¤ë‹¨ê³„ ì¶”ë¡  (ë§¤ìš° ì¤‘ìš”!)\n"
        "ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì „ì— **ë‹¨ê³„ë³„ë¡œ ìƒê°**í•˜ì„¸ìš”. íŠ¹íˆ ë‹¤ìŒ ê²½ìš°ì— ì£¼ì˜í•˜ì„¸ìš”:\n\n"
        "### ê²½ë¡œ/ë‹¨ê³„ ì§ˆë¬¸\n"
        "- 'Aì—ì„œ Bê°€ ë˜ë ¤ë©´?', 'Aì—ì„œ Bê¹Œì§€ ì–¼ë§ˆë‚˜?'ì™€ ê°™ì€ ì§ˆë¬¸ì€ **ì¤‘ê°„ ë‹¨ê³„**ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
        "- ì˜ˆ: 'ëŒ€ë¦¬ì—ì„œ ë¶€ì¥ì´ ë˜ë ¤ë©´?' â†’ ëŒ€ë¦¬â†’ê³¼ì¥â†’ì°¨ì¥â†’ë¶€ì¥ ê° ë‹¨ê³„ì˜ ê¸°ê°„ì„ **ëª¨ë‘ í•©ì‚°**í•´ì•¼ í•´ìš”.\n\n"
        "### ê³„ì‚°ì´ í•„ìš”í•œ ì§ˆë¬¸\n"
        "- ì—¬ëŸ¬ ê°’ì„ ë”í•˜ê±°ë‚˜ ì¡°ê±´ì„ ì¡°í•©í•´ì•¼ í•˜ëŠ” ê²½ìš°, **ê³„ì‚° ê³¼ì •ì„ ëª…ì‹œ**í•˜ì„¸ìš”.\n"
        "- ì˜ˆ: 'ìµœì†Œ ëª‡ ë…„?' â†’ 'ëŒ€ë¦¬â†’ê³¼ì¥ 4ë…„ + ê³¼ì¥â†’ë¶€ì¥ 4ë…„ = **ì´ 8ë…„**'\n\n"
        "### ì¡°ê±´ ì¡°í•© ì§ˆë¬¸\n"
        "- '~í•˜ë©´ì„œ ~í•˜ë ¤ë©´?'ì²˜ëŸ¼ ì—¬ëŸ¬ ì¡°ê±´ì´ ìˆìœ¼ë©´ **ëª¨ë“  ì¡°ê±´ì„ í™•ì¸**í•˜ì„¸ìš”.\n"
        "- ë¬¸ì„œì˜ í‘œë‚˜ ëª©ë¡ì—ì„œ ê´€ë ¨ëœ **ëª¨ë“  í–‰/í•­ëª©**ì„ ê²€í† í•˜ì„¸ìš”.\n\n"
        "### ì¶”ë¡  ì˜ˆì‹œ\n"
        "ì§ˆë¬¸: 'ì‚¬ì›ì—ì„œ ê³¼ì¥ì´ ë˜ë ¤ë©´ ìµœì†Œ ëª‡ ë…„ì´ í•„ìš”í•œê°€ìš”?'\n"
        "ì¶”ë¡  ê³¼ì •:\n"
        "1. ë¬¸ì„œì—ì„œ ìŠ¹ì§„ ë‹¨ê³„ í™•ì¸: ì‚¬ì› â†’ ëŒ€ë¦¬ â†’ ê³¼ì¥\n"
        "2. ê° ë‹¨ê³„ë³„ ì†Œìš” ê¸°ê°„: ì‚¬ì›â†’ëŒ€ë¦¬ 3ë…„, ëŒ€ë¦¬â†’ê³¼ì¥ 4ë…„\n"
        "3. í•©ì‚°: 3ë…„ + 4ë…„ = **7ë…„**\n"
        "ë‹µë³€: 'ì‚¬ì›ì—ì„œ ê³¼ì¥ì´ ë˜ë ¤ë©´ ìµœì†Œ **7ë…„**ì´ í•„ìš”í•´ìš”. (ì‚¬ì›â†’ëŒ€ë¦¬ 3ë…„ + ëŒ€ë¦¬â†’ê³¼ì¥ 4ë…„)'\n\n"

        "## ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ ê°€ì´ë“œ\n"
        "- **ì²« ë¬¸ì¥**: í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì¹œê·¼í•˜ê²Œ ì•Œë ¤ë“œë ¤ìš”\n"
        "- **ëª©ë¡**: ì—¬ëŸ¬ í•­ëª©ì€ `-` ë˜ëŠ” `1.`ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ìš”\n"
        "- **ê°•ì¡°**: ì¤‘ìš”í•œ ìˆ«ìë‚˜ í•µì‹¬ ë‚´ìš©ë§Œ **êµµê²Œ** í‘œì‹œí•´ìš”\n"
        "- **í‘œ**: ë¹„êµê°€ í•„ìš”í•˜ë©´ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ìš”\n"
    )

    # ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ìˆìœ¼ë©´ ì´ë¯¸ì§€ ì‚½ì… ì•ˆë‚´ ì¶”ê°€
    if image_refs:
        image_guide = (
            "\n## ğŸš¨ ì›ë³¸ ì´ë¯¸ì§€ ì‚½ì… (ë§¤ìš° ì¤‘ìš” - í•„ìˆ˜!)\n"
            "ì œê³µëœ ë¬¸ì„œì— í‘œë‚˜ ê·¸ë¦¼ì˜ **ì›ë³¸ ì´ë¯¸ì§€**ê°€ ìˆìŠµë‹ˆë‹¤.\n"
            "**ë°˜ë“œì‹œ ë‹µë³€ ë³¸ë¬¸ì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…í•´ì•¼ í•©ë‹ˆë‹¤!** ì´ë¯¸ì§€ ì—†ì´ ë‹µë³€í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n\n"
            "### ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€:\n"
        )
        for i, img in enumerate(image_refs, start=1):
            img_type_ko = "í‘œ" if img["type"] == "table" else "ê·¸ë¦¼"
            page_info = f" (p.{img['page']})" if img.get("page") else ""
            image_guide += f"- [IMG{i}]: {img['doc_title']}{page_info}ì˜ {img_type_ko}\n"

        image_guide += (
            "\n### ì´ë¯¸ì§€ ì‚½ì… í˜•ì‹:\n"
            "ë‹µë³€ ë³¸ë¬¸ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì„¤ëª…í•  ë•Œ **ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ ì‚½ì…**í•˜ì„¸ìš”:\n\n"
            "```\n"
            "![í‘œ1: ì„¤ëª…][IMG1]\n"
            "```\n\n"
            "### âœ… ì˜¬ë°”ë¥¸ ë‹µë³€ ì˜ˆì‹œ (ì´ë¯¸ì§€ í¬í•¨):\n"
            "```\n"
            "ì¸ì‚¬í‰ê°€ í•­ëª©ê³¼ ë¹„ìœ¨ì„ ì•ˆë‚´í•´ ë“œë¦´ê²Œìš”!\n"
            "\n"
            "![í‘œ1: ì¸ì‚¬í‰ê°€ í•­ëª© ë° ë¹„ìœ¨][IMG1]\n"
            "\n"
            "ìœ„ í‘œë¥¼ ë³´ì‹œë©´ í‰ê°€ í•­ëª©ë³„ ë¹„ìœ¨ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”.\n"
            "```\n\n"
            "### âŒ ì˜ëª»ëœ ë‹µë³€ ì˜ˆì‹œ (ì´ë¯¸ì§€ ë¯¸í¬í•¨):\n"
            "```\n"
            "ì¸ì‚¬í‰ê°€ í•­ëª©ê³¼ ë¹„ìœ¨ì„ ì•ˆë‚´í•´ ë“œë¦´ê²Œìš”!\n"
            "- ì—…ì í‰ê°€: 50%\n"
            "- ì—­ëŸ‰í‰ê°€: 30%\n"
            "...(ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ë‚˜ì—´ - ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ë¨!)\n"
            "```\n\n"
            "**í•„ìˆ˜ ê·œì¹™**:\n"
            "1. í‘œ/ê·¸ë¦¼ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ **ë°˜ë“œì‹œ** `![í‘œN: ì„¤ëª…][IMGë²ˆí˜¸]` í˜•ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ì‚½ì…\n"
            "2. `[IMG1]`, `[IMG2]` ë“±ì˜ ì°¸ì¡° IDë¥¼ **ì •í™•íˆ ê·¸ëŒ€ë¡œ** ì‚¬ìš©\n"
            "3. URLì„ ì§ì ‘ ì‘ì„±í•˜ì§€ ë§ê³  ì°¸ì¡° IDë§Œ ì‚¬ìš©\n"
            "4. ì´ë¯¸ì§€ ì‚½ì… í›„ 'ìœ„ í‘œ/ê·¸ë¦¼ì„ ì°¸ê³ í•´ ì£¼ì„¸ìš”'ë¼ê³  ì•ˆë‚´\n"
        )
        base_prompt += image_guide
    else:
        base_prompt += "\n"

    base_prompt += (
        "\n## ì •ë³´ ì—†ìŒ ì‘ë‹µ ì˜ˆì‹œ\n"
        "ì§ˆë¬¸: 'íœ´ê°€ ì‹ ì²­ ë°©ë²•ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'\n"
        "ë‹µë³€:\n"
        "íœ´ê°€ ì‹ ì²­ ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì…¨êµ°ìš”!\n\n"
        "ì•„ì‰½ê²Œë„ ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” íœ´ê°€ ì‹ ì²­ ë°©ë²•ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”.\n\n"
        "ì •í™•í•œ ì •ë³´ë¥¼ ìœ„í•´ **ë‹´ë‹¹ ë¶€ì„œì— ë¬¸ì˜**í•´ ë³´ì‹œë©´ ìì„¸í•œ ì•ˆë‚´ë¥¼ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n\n"
        "âš ï¸ **ì¤‘ìš”**: ë¬¸ì„œì— ì—†ëŠ” ë‹´ë‹¹ì ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“±ì„ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”!\n\n"

        "## ì •ë³´ ìˆìŒ ì‘ë‹µ ì˜ˆì‹œ\n"
        "ì§ˆë¬¸: 'ì—°ì°¨ íœ´ê°€ ì¼ìˆ˜ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'\n"
        "ë‹µë³€:\n"
        "ì—°ì°¨ íœ´ê°€ ì¼ìˆ˜ê°€ ê¶ê¸ˆí•˜ì…¨êµ°ìš”!\n\n"
        "ë¬¸ì„œì— ë”°ë¥´ë©´ ì—°ì°¨ íœ´ê°€ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶€ì—¬ë¼ìš”:\n"
        "- 1ë…„ ë¯¸ë§Œ ê·¼ë¬´: ì›” 1ì¼ì”©\n"
        "- 1ë…„ ì´ìƒ ê·¼ë¬´: 15ì¼\n"
        "- 3ë…„ ì´ìƒ ê·¼ë¬´: ë§¤ 2ë…„ë§ˆë‹¤ 1ì¼ ì¶”ê°€\n\n"
        "ë” ìì„¸í•œ ë‚´ìš©ì€ ë¬¸ì„œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”!"
    )

    return base_prompt


async def generate_answer(
    question: str, candidates: List[Union[ScoredChunk, Chunk]]
) -> Tuple[str, List[Chunk]]:
    """
    ì§ˆë¬¸ + í›„ë³´ ì²­í¬ë“¤ë¡œ ë‹µë³€ ìƒì„± (ë¹„ë™ê¸° + ë™ì‹œì„± ì œì–´).
    ë°˜í™˜: (answer_text, used_chunks)
    """
    used_chunks: List[Chunk] = _select_chunks(candidates, max_chars=6000)
    context, image_refs = _build_context(used_chunks)

    system_msg = _get_system_prompt(image_refs)

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì°¸ì¡° ID ì•ˆë‚´ ì¶”ê°€
    image_info = ""
    if image_refs:
        image_info = "\n\nğŸ–¼ï¸ [ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ - ë°˜ë“œì‹œ ë‹µë³€ì— í¬í•¨í•  ê²ƒ!]\n"
        for i, img in enumerate(image_refs, start=1):
            img_type_ko = "í‘œ" if img["type"] == "table" else "ê·¸ë¦¼"
            image_info += f"- [IMG{i}]: {img['doc_title']}ì˜ {img_type_ko}\n"
        image_info += (
            "\nâš ï¸ ìœ„ ì´ë¯¸ì§€ë¥¼ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë“œì‹œ ì‚½ì…í•˜ì„¸ìš”!\n"
            "í˜•ì‹: ![í‘œ1: ì„¤ëª…][IMG1]\n"
        )

    user_msg = (
        f"ì§ˆë¬¸:\n{question}\n\n"
        f"ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë“¤ì´ë‹¤. "
        f"ì´ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ë¼.\n\n"
        f"{context}"
        f"{image_info}"
    )

    # ë¹„ë™ê¸° Chat Completions (ë™ì‹œì„± ì œì–´ í¬í•¨)
    # ë‹µë³€ ìƒì„±ì€ ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•˜ë¯€ë¡œ ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©
    resp = await call_chat_completion_async(
        model=settings.openai_advanced_model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.2,
    )
    answer = resp.choices[0].message.content.strip() if resp.choices else ""

    # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ë§Œ í•„í„°ë§
    actually_used_chunks = _filter_actually_used_chunks(answer, used_chunks)

    return answer, actually_used_chunks


async def generate_answer_stream(
    question: str, candidates: List[Union[ScoredChunk, Chunk]]
) -> AsyncIterator[Tuple[str, List[Chunk] | None, List[Dict[str, Any]] | None]]:
    """
    ì§ˆë¬¸ + í›„ë³´ ì²­í¬ë“¤ë¡œ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (ë¹„ë™ê¸° + ë™ì‹œì„± ì œì–´).

    ì¤„ ë‹¨ìœ„ ë²„í¼ë§: ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ ì•ˆì •ì„±ì„ ìœ„í•´ ì¤„ë°”ê¿ˆ(\n) ê¸°ì¤€ìœ¼ë¡œ ë²„í¼ë§ í›„ ì „ì†¡.
    - ì™„ì „í•œ ì¤„ì´ ë˜ë©´ ì „ì†¡
    - ì¤„ë°”ê¿ˆ ì—†ì´ 50ì ì´ìƒ ëˆ„ì ë˜ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì „ì†¡ (ê¸´ ë¬¸ì¥ ëŒ€ì‘)

    Yields:
        (token, None, None) - ì¤„ ë‹¨ìœ„ ë˜ëŠ” ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°
        ("", used_chunks, image_refs) - ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì°¸ì¡° (ìŠ¤íŠ¸ë¦¼ ë)
    """
    used_chunks: List[Chunk] = _select_chunks(candidates, max_chars=6000)
    context, image_refs = _build_context(used_chunks)

    system_msg = _get_system_prompt(image_refs)

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì°¸ì¡° ID ì•ˆë‚´ ì¶”ê°€
    image_info = ""
    if image_refs:
        image_info = "\n\nğŸ–¼ï¸ [ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ - ë°˜ë“œì‹œ ë‹µë³€ì— í¬í•¨í•  ê²ƒ!]\n"
        for i, img in enumerate(image_refs, start=1):
            img_type_ko = "í‘œ" if img["type"] == "table" else "ê·¸ë¦¼"
            image_info += f"- [IMG{i}]: {img['doc_title']}ì˜ {img_type_ko}\n"
        image_info += (
            "\nâš ï¸ ìœ„ ì´ë¯¸ì§€ë¥¼ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë“œì‹œ ì‚½ì…í•˜ì„¸ìš”!\n"
            "í˜•ì‹: ![í‘œ1: ì„¤ëª…][IMG1]\n"
        )

    user_msg = (
        f"ì§ˆë¬¸:\n{question}\n\n"
        f"ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë“¤ì´ë‹¤. "
        f"ì´ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ë¼.\n\n"
        f"{context}"
        f"{image_info}"
    )

    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° (ë™ì‹œì„± ì œì–´ + ìë™ Semaphore í•´ì œ)
    # ë‹µë³€ ìƒì„±ì€ ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•˜ë¯€ë¡œ ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©
    stream = await call_chat_completion_stream_async(
        model=settings.openai_advanced_model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.2,
    )

    # ì¤„ ë‹¨ìœ„ ë²„í¼ë§
    line_buffer = ""
    full_answer = ""  # ì „ì²´ ë‹µë³€ ìˆ˜ì§‘ (ì²­í¬ í•„í„°ë§ìš©)
    FLUSH_THRESHOLD = 50  # ì¤„ë°”ê¿ˆ ì—†ì´ ì´ ê¸¸ì´ ì´ˆê³¼ ì‹œ ê°•ì œ ì „ì†¡

    try:
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                line_buffer += token
                full_answer += token  # ì „ì²´ ë‹µë³€ ìˆ˜ì§‘

                # ì¤„ë°”ê¿ˆì´ ìˆìœ¼ë©´ ì™„ì „í•œ ì¤„ë“¤ì„ ì „ì†¡
                while '\n' in line_buffer:
                    line, line_buffer = line_buffer.split('\n', 1)
                    yield (line + '\n', None, None)

                # ì¤„ë°”ê¿ˆ ì—†ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë‹¨ì–´ ê²½ê³„ì—ì„œ ì „ì†¡
                if len(line_buffer) > FLUSH_THRESHOLD:
                    last_space = line_buffer.rfind(' ')
                    if last_space > 0:
                        yield (line_buffer[:last_space + 1], None, None)
                        line_buffer = line_buffer[last_space + 1:]
    except Exception as e:
        log.error(f"[GENERATOR] Streaming error: {e}")
        raise

    # ë‚¨ì€ ë²„í¼ ì „ì†¡
    if line_buffer:
        yield (line_buffer, None, None)

    # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ë§Œ í•„í„°ë§
    actually_used_chunks = _filter_actually_used_chunks(full_answer, used_chunks)

    # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì°¸ì¡° ë°˜í™˜
    # image_refsë¥¼ [IMG1], [IMG2] í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_image_refs = []
    for i, img in enumerate(image_refs, start=1):
        formatted_image_refs.append({
            "ref": f"[IMG{i}]",
            "url": img["url"],
            "type": img["type"],
            "doc_title": img.get("doc_title"),
            "page": img.get("page"),
        })

    yield ("", actually_used_chunks, formatted_image_refs)
